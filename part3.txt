Main Contribution
The main contribution of this paper is the introduction of the Word-in-Context (WiC) dataset, a large-scale benchmark for evaluating context-sensitive meaning representations. 
The authors argue that existing datasets like Stanford Contextual Word Similarity are limited in evaluating the dynamic semantics of words. WiC addresses this by providing expert-annotated pairs of contexts with the same word, where the task is to identify if the word has the same meaning across the two contexts or not.

Technical Ideas
The paper does not introduce a new model or algorithm. Instead, it presents WiC as a novel evaluation benchmark framed as a binary classification task. The key idea is to pair identical words in different contexts, such that a context-insensitive word embedding model would perform similarly to a random baseline. The dataset is constructed using high-quality annotations curated by experts.

Evaluation Setup
The evaluation setup involves testing various context-sensitive word representation techniques, such as sense embeddings, contextualized word embeddings, and word sense disambiguation systems, on the WiC dataset. 
The research question is to assess the capability of these models in capturing the dynamic semantics of words across different contexts. The authors claim that WiC provides a more suitable benchmark than existing datasets for this purpose.

Strengths, Weaknesses, and Future Directions
One strength of this paper is the creation of a high-quality dataset that addresses a crucial limitation in evaluating context-sensitive word representations. A potential weakness is the binary nature of the task, which may not fully capture the nuances of word meanings across contexts.

If given 1-2 months, one could explore the performance of state-of-the-art language models on the WiC dataset and analyze their strengths and weaknesses. With 1 year, the dataset could be extended to include more languages and domains. Over 5 years, the ideas could be expanded to develop more comprehensive evaluation benchmarks that capture various aspects of context-sensitive meaning representations.